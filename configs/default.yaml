# Model Architecture
model:
  N: 6
  d_model: 512
  d_ff: 2048
  n_heads: 8
  dropout_prob: 0.1
  max_padding: 20

# Training Configuration
training:
  batch_size: 400
  shuffle: true
  epochs: 20
  learning_rate:
    base: 1.0
    warmup_steps: 3000
  accumulation_steps: 10
  random_seed: 101
  label_smoothing: 0.1
# Dataset Configurations
dataset:
  name: "wmt24"  # Options: wmt14, m30k, wmt24
  language_pair: ["de", "en"]
  size: 5000000  # max dataset size to use # TODO: rename to max_size
  cache: true    # whether to cache preprocessed data
  download: true
  preprocess: true

# Tokenizer Configuration
tokenizer:
  type: "bert"   # Options: bert, spacy
  cache: true    # whether to cache tokenizers

# Logging Configuration
logging:
  experiment_name: "transformer_training"
  tracking_uri: "http://127.0.0.1:8080"
  artifacts_dir: "artifacts"
  subdirs:
    models: "saved_models"
    data: "saved_data"
    tokenizers: "saved_tokenizers"
    loss_curves: "loss_curves"

# Hardware Configuration
hardware:
  device: "cuda"  # Options: cuda, cpu
  data_parallel: true  # whether to use DataParallel for multi-GPU training

# Translation Configuration
translation:
  batch_limit: 1000

extras: # extra config options
  harvard: false
